fp32-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: fp32
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'REALTIME'
    TORCH_INDUCTOR: '0'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
fp32-throughput-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: fp32
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'THROUGHPUT'
    TORCH_INDUCTOR: '0'
    BATCH_SIZE: '120'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
bf32-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: bf32
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'REALTIME'
    TORCH_INDUCTOR: '0'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
bf32-throughput-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: bf32
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'THROUGHPUT'
    TORCH_INDUCTOR: '0'
    BATCH_SIZE: '120'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
int8-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: int8
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'REALTIME'
    TORCH_INDUCTOR: '0'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
int8-throughput-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: int8
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'THROUGHPUT'
    TORCH_INDUCTOR: '0'
    BBATCH_SIZE: '120'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
bf16-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: bf16
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'REALTIME'
    TORCH_INDUCTOR: '0'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
bf16-throughput-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: bf16
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'THROUGHPUT'
    TORCH_INDUCTOR: '0'
    BATCH_SIZE: '120'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
fp16-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: fp16
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'REALTIME'
    TORCH_INDUCTOR: '0'
    DNNL_MAX_CPU_ISA: AVX512_CORE_AMX_FP16
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
fp16-throughput-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-inference
  cmd: bash run_model.sh
  cap_add: 'SYS_NICE'
  env:
    PRECISION: fp16
    EVAL_DATA_FILE: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    OUTPUT_DIR: /tmp
    FINETUNED_MODEL: /pytorch/bert_squad/models/bert_squad_model
    TEST_MODE: 'THROUGHPUT'
    TORCH_INDUCTOR: '0'
    DNNL_MAX_CPU_ISA: AVX512_CORE_AMX_FP16
    BATCH_SIZE: '120'
  volumes:
    - src: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
      dst: /pytorch/bert_squad/models/bert_squad_model/dev-v1.1.json
    - src: /pytorch/bert_squad/models/bert_squad_model
      dst: /pytorch/bert_squad/models/bert_squad_model
    - src: /tmp
      dst: /tmp
